{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLANTILLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************** Variables Fuente ***********************************************************\n",
    "nombre_archivo = 'transacciones_diccionario-trx-emp_ods_H201910-202411.csv'  # Nombre del archivo, ej:'Archivo Original.xlsx'\n",
    "periodo = 'H201910-202411' # Periodo del archivo, ej:M202102\n",
    "\n",
    "path = 'D:/Fuentes/Planos/Diccionario-TRX/' # Carpeta raiz de la fuente\n",
    "nombre_diccionario = 'dict_meta_transacciones_diccionario-trx-emp_ods.xlsx' # Nombre del diccionario de datos\n",
    "\n",
    "file_path = path + 'Originales/Empresas/' + nombre_archivo\n",
    "file_path_dict = 'D:/Fuentes/Planos/Diccionarios/' + nombre_diccionario\n",
    "\n",
    "rename_columns = {'' : ''\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from calendar import monthrange\n",
    "from datetime import datetime\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows',50)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#Función carga archivo en df con columnas en minuscula\n",
    "def read_file(file_path:str) -> pd.DataFrame:\n",
    "    if os.path.splitext(file_path)[1] == '.xlsx':\n",
    "        df = pd.read_excel(file_path,\n",
    "            skiprows = None,\n",
    "            decimal = ',',\n",
    "            dtype = 'str'\n",
    "            )\n",
    "    else:\n",
    "        df = pd.read_csv(file_path,\n",
    "            sep = '|',\n",
    "            decimal = ',',\n",
    "            encoding = 'utf-8',\n",
    "#            encoding = 'ISO-8859-1',\n",
    "            low_memory = False,\n",
    "            quoting = 3,\n",
    "            error_bad_lines = False,\n",
    "            dtype = 'str'\n",
    "            )\n",
    "    list_columns = df.columns.to_list()\n",
    "    list_new_columns = []\n",
    "    for item in list_columns:\n",
    "        item = str(item)\n",
    "        item = item.replace('-', '')\n",
    "        item = item.replace(':', '')\n",
    "        item = item.replace('/', '')\n",
    "        item = item.replace('.', '')\n",
    "        item = item.replace(',', '')\n",
    "        item = item.replace('\\n', ' ')\n",
    "        item = item.replace('\\t', ' ')\n",
    "        item = item.replace('  ', ' ')\n",
    "        item = item.strip()\n",
    "        item = item.replace(' ', '_')\n",
    "        item = item.lower()\n",
    "        item = item.replace('á', 'a')\n",
    "        item = item.replace('é', 'e')\n",
    "        item = item.replace('í', 'i')\n",
    "        item = item.replace('ó', 'o')\n",
    "        item = item.replace('ú', 'u')\n",
    "        list_new_columns.append(item)\n",
    "    df.columns = list_new_columns\n",
    "    df = df.rename(columns = rename_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### generar_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_load(file_path:str):   \n",
    "    df = pd.read_csv(file_path, \n",
    "                  sep = '|', \n",
    "                  decimal = ',',\n",
    "                  encoding = 'utf-8',\n",
    "                  low_memory = False,\n",
    "                  quoting = 3,\n",
    "                  error_bad_lines = False,\n",
    "                ) \n",
    "    entity = 'BOCC'\n",
    "    filename = os.path.basename(file_path)\n",
    "    split = filename.split('_')\n",
    "    domain = split[0]\n",
    "    subdomain = split[1]\n",
    "    source = split[2]\n",
    "    periodo = split[3].split('.')[0]\n",
    "    len_period = len(periodo)\n",
    "    first_letter = periodo[0:1]\n",
    "    extencion = split[3].split('.')[1]\n",
    "    bucket_name_dl = 'data-bocc-pro-landing'\n",
    "    generation_date = generation_date = str(datetime.now())[0:10]\n",
    "    #generation_date = '2021-02-22'\n",
    "    num_rows = str(len(df.axes[0])+1)\n",
    "    size = str(os.path.getsize(file_path))\n",
    "    version = '0-0'\n",
    "    meta_file = domain + '_' + subdomain + '_' + source + '_v' + version + '.meta'  \n",
    "      \n",
    "    if (first_letter == \"D\" and len_period == 9): # Diario Incremental\n",
    "        start_period = periodo[1:5] + '-' +  periodo[5:7] + '-' +  periodo[7:9]\n",
    "        end_period = periodo[1:5] + '-' +  periodo[5:7] + '-' +  periodo[7:9]\n",
    "        frequency = 'DAILY'\n",
    "    elif (first_letter == \"H\" and len_period == 18): # Diario Historico\n",
    "        start_period = periodo[1:5] + '-' +  periodo[5:7] + '-' +  periodo[7:9]\n",
    "        end_period = periodo[10:14] + '-' +  periodo[14:16] + '-' +  periodo[16:18]\n",
    "        frequency = 'HISTORICAL'\n",
    "    elif (first_letter == \"M\" and len_period == 7): # Mensual Incremental\n",
    "        start_period = periodo[1:5] + '-' +  periodo[5:7] + '-' + '01'\n",
    "        end_period = periodo[1:5] + '-' +  periodo[5:7] + '-' + str(monthrange(int(periodo[1:5]), int(periodo[5:7]))[1])\n",
    "        frequency = 'MONTHLY'   \n",
    "    elif (first_letter == \"H\" and len_period == 14): # Mensual Historico\n",
    "        start_period = periodo[1:5] + '-' +  periodo[5:7]\n",
    "        end_period = periodo[8:12] + '-' +  periodo[12:14]\n",
    "        frequency = 'HISTORICAL'\n",
    "    elif (first_letter == \"F\" and len_period == 8): # Quincenal\n",
    "        if periodo[-1:] == '1':\n",
    "            start_period = periodo[1:5] + '-' +  periodo[5:7] + '-' + '01'\n",
    "            end_period = periodo[1:5] + '-' +  periodo[5:7] + '-' + '15'\n",
    "        elif periodo[-1:] == '2':\n",
    "            start_period = periodo[1:5] + '-' +  periodo[5:7] + '-' + '16'\n",
    "            end_period = periodo[1:5] + '-' +  periodo[5:7] + '-' + str(monthrange(int(periodo[1:5]), int(periodo[5:7]))[1])\n",
    "        frequency = 'BIWEEKLY'  \n",
    "    elif (first_letter == \"Q\" and len_period == 6): # Trimestral Q20204\n",
    "        if periodo[-1:] == '1':\n",
    "            start_period = periodo[1:5] + '-' +  '01' + '-' + '01'\n",
    "            end_period = periodo[1:5] + '-' +  '03' + '-' + str(monthrange(int(periodo[1:5]), 3)[1])\n",
    "        elif periodo[-1:] == '2':\n",
    "            start_period = periodo[1:5] + '-' +  '04' + '-' + '01'\n",
    "            end_period = periodo[1:5] + '-' +  '06' + '-' + str(monthrange(int(periodo[1:5]), 6)[1])\n",
    "        elif periodo[-1:] == '3':\n",
    "            start_period = periodo[1:5] + '-' +  '07' + '-' + '01'\n",
    "            end_period = periodo[1:5] + '-' +  '09' + '-' + str(monthrange(int(periodo[1:5]), 9)[1])\n",
    "        elif periodo[-1:] == '4':\n",
    "            start_period = periodo[1:5] + '-' +  '10' + '-' + '01'\n",
    "            end_period = periodo[1:5] + '-' +  '12' + '-' + str(monthrange(int(periodo[1:5]), 12)[1])     \n",
    "        frequency = 'QUATERLY'\n",
    "    else:\n",
    "        print(\"ERROR: NOMENCLATURA DE PERIODO\", periodo)\n",
    "      \n",
    "    datos = '{\\n' + '\\t' + '\\\n",
    "\"entity\": ' + '\"' + entity + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"filename\": ' + '\"' + filename + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"domain\": ' + '\"' + domain + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"subdomain\": ' + '\"' + subdomain + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"source\": ' + '\"' + source + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"bucket_name_dl\": ' + '\"' + bucket_name_dl + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"generation_date\": ' + '\"' + generation_date + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"start_period\": ' + '\"' + start_period + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"end_period\": ' + '\"' + end_period + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"frequency\": ' + '\"' + frequency + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"num_rows\": ' + str(num_rows) + ',\\n' + '\\t' + '\\\n",
    "\"size\": ' + '\"' + size + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"meta_file\": ' + '\"' + meta_file + '\"' + ',\\n' + '\\t' + '\\\n",
    "\"version\": ' + '\"' + version + '\"' + '\\n' +  '\\\n",
    "}'\n",
    "                            \n",
    "    file_load = file_path[0:-4] + '.load'\n",
    "    file = open(file_load,'w')\n",
    "    file.write(datos)\n",
    "    file.close() \n",
    "    print('\\t', file_load)\n",
    "    print(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### read_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meta(file_path_dicc):\n",
    "    df_dicc = pd.read_excel(file_path_dicc, sheet_name = 'NOMBRE_TABLA_FIELDS', skiprows = 1)\n",
    "    df_dicc = df_dicc[['FIELD_NAME', 'DATA_TYPE', 'FORMAT', 'LENGTH', 'PRECISION', 'SCALE']]\n",
    "    df_dicc['length'] = df_dicc['LENGTH'].astype('Int64')\n",
    "    df_dicc['precision'] = df_dicc['PRECISION'].astype('Int64')\n",
    "    df_dicc['SCALE'] = df_dicc.SCALE.fillna(0)\n",
    "    df_dicc['scale'] = df_dicc['SCALE'].astype('Int64')\n",
    "    df_dicc['type_data'] = df_dicc['DATA_TYPE']\n",
    "    df_dicc['type_dict'] = np.where(df_dicc['type_data'] == 'string', \n",
    "                                    df_dicc['type_data'] + '(' + df_dicc['length'].astype(str) + ')', np.nan)\n",
    "    df_dicc['type_dict'] = np.where(df_dicc['type_data'] == 'number', \n",
    "                                    df_dicc['type_data'] + '(' + df_dicc['precision'].astype(str) + ',' +  \n",
    "                                    df_dicc['scale'].astype(str) + ')' , df_dicc['type_dict'])\n",
    "    df_dicc['type_dict'] = np.where(df_dicc['type_data'] == 'date', \n",
    "                                    df_dicc['type_data'] + '(' + df_dicc['FORMAT'].astype(str) + ')', \n",
    "                                    df_dicc['type_dict'])\n",
    "    df_dicc['type_np'] = np.where((df_dicc['DATA_TYPE'] == 'number') & (df_dicc['scale'] > 0), 'float64',\n",
    "                                np.where((df_dicc['DATA_TYPE'] == 'number') & (df_dicc['scale'] == 0), 'Int64',\n",
    "                                    np.where((df_dicc['DATA_TYPE'] == 'string'), 'object',\n",
    "                                             'datetime64[ns]')))\n",
    "    \n",
    "    df_dicc['type'] = np.where((df_dicc['DATA_TYPE'] == 'number') & (df_dicc['scale'] > 0), 'float',\n",
    "                                np.where((df_dicc['DATA_TYPE'] == 'number') & (df_dicc['scale'] == 0), 'integer',\n",
    "                                    np.where((df_dicc['DATA_TYPE'] == 'string'), 'string',\n",
    "                                             'date')))\n",
    "    \n",
    "    df_dicc = df_dicc[['FIELD_NAME', 'type_data','type_dict','type_np', 'type', 'length', 'precision','scale' ]]     \n",
    "    df_meta = pd.read_excel(file_path_dicc, sheet_name = 'NOMBRE_TABLA_DESC', skiprows = 1)\n",
    "    \n",
    "    dict_meta = {}\n",
    "    \n",
    "    dict_meta['dominio'] = df_meta['domain'].loc[0]\n",
    "    dict_meta['subdominio'] = df_meta['subdomain'].loc[0]\n",
    "    dict_meta['fuente'] = df_meta['source'].loc[0]\n",
    "    dict_meta['extension'] = '.' + df_meta['file_format'].loc[0]\n",
    "    dict_meta['cols_string'] = df_dicc[(df_dicc['type_data'] == 'string') & \n",
    "                                       (df_dicc['scale'] == 0)].FIELD_NAME.tolist()\n",
    "    dict_meta['cols_float'] = df_dicc[(df_dicc['type_data'] == 'number') & \n",
    "                                      (df_dicc['scale'] > 0)].FIELD_NAME.tolist()\n",
    "    dict_meta['cols_integer'] = df_dicc[(df_dicc['type_data'] == 'number') & \n",
    "                                        (df_dicc['scale'] == 0)].FIELD_NAME.tolist()\n",
    "    dict_meta['cols_date'] = df_dicc[df_dicc['type_data'] == 'date'].FIELD_NAME.tolist()\n",
    "    dict_meta['cols'] = df_dicc.set_index('FIELD_NAME').T.to_dict('dict')  \n",
    "    \n",
    "    dict_meta['list_cols'] = []\n",
    "   \n",
    "\n",
    "    for i in df_dicc.index:\n",
    "        dict_row = {'name': df_dicc['FIELD_NAME'][i], \n",
    "                    'type_np': df_dicc['type_np'][i], \n",
    "                    'type_dict': df_dicc['type_dict'][i],\n",
    "                    'type': df_dicc['type'][i]}\n",
    "    \n",
    "    return dict_meta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### validacion_columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion_columnas(df_original): # valida que el archivo contenga las mismas columnas del diccionario\n",
    "    validacion = {}\n",
    "    validacion['list_cols_dict'] = list(meta['cols'].keys())\n",
    "    validacion['list_cols_adicionales'] = list(set(df_original.columns) - set(validacion['list_cols_dict']))\n",
    "    validacion['list_cols_faltantes'] = list(set(validacion['list_cols_dict'])  - set(df_original.columns))\n",
    "    if len(validacion['list_cols_faltantes']) == 0: \n",
    "        validacion['list_cols_faltantes_check'] = True \n",
    "    else: \n",
    "        validacion['list_cols_faltantes_check'] = False\n",
    "    if len(validacion['list_cols_adicionales']) == 0: \n",
    "        validacion['list_cols_adicionales_check'] = True \n",
    "    else: \n",
    "        validacion['list_cols_adicionales_check'] = False \n",
    "    return validacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### transformacion_datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformacion_datos(df_original, meta):\n",
    "    columns = list(meta['cols'].keys())\n",
    "    try:\n",
    "        df_final = df_original[columns] # WARNING\n",
    "    except:\n",
    "        df_final = df_original\n",
    "        print('\\t', 'error columna faltante - no se encuentra campo:', sys.exc_info()[1] )\n",
    "#    df_final.loc[:columns] = df_original[columns]\n",
    "    num_error_tipo = 0\n",
    "\n",
    "# Cambios de Tipo - Conversión a Entero\n",
    "    for campo in meta['cols_integer']:\n",
    "        try:\n",
    "            df_final.loc[:, campo] = pd.to_numeric(df_final.loc[:, campo], errors = 'raise')\n",
    "            df_final.loc[:, campo] = df_final.loc[:, campo].astype('Int64')\n",
    "        except:\n",
    "            num_error_tipo = num_error_tipo + 1\n",
    "            print('\\t', 'error tipo de dato (integer) - no se puede convertir campo:', campo, sys.exc_info()[1])\n",
    "        \n",
    "# Cambios de Tipo - Conversión y Redondeo a Float\n",
    "    for campo in meta['cols_float']:\n",
    "        try:\n",
    "            df_final.loc[:, campo] = pd.to_numeric(df_final.loc[:, campo], errors = 'raise')\n",
    "            df_final.loc[:, campo] = df_final.loc[:, campo].astype('float64')\n",
    "            df_final.loc[:, campo] = pd.Series([round(val,meta['cols'][campo]['scale']) for val in df_final[campo]])\n",
    "        except:\n",
    "            num_error_tipo = num_error_tipo + 1\n",
    "            print('\\t', 'error tipo de dato (float) - no se puede convertir campo:', campo, sys.exc_info()[1])\n",
    "    \n",
    "# Cambios de Tipo - Conversión a Fecha\n",
    "    for campo in meta['cols_date']:\n",
    "        try:\n",
    "            df_final.loc[:, campo] = pd.to_datetime(df_final[campo], format='%Y-%m-%d %H:%M:%S')\n",
    "            df_final.loc[:, campo] = df_final[campo].dt.strftime('%d/%m/%Y')\n",
    "            df_final.loc[:, campo] = pd.to_datetime(df_final[campo], format='%d/%m/%Y')\n",
    "        except:\n",
    "            num_error_tipo = num_error_tipo + 1\n",
    "            print('\\t', 'error tipo de dato (date) - no se puede convertir campo:', campo, sys.exc_info()[1])\n",
    "        \n",
    "# Cambios de Tipo - Transformación Strings\n",
    "    for campo in meta['cols_string']:\n",
    "        try:\n",
    "            df_final.loc[:, campo] = df_final[campo].str.replace('\\n',' ')\n",
    "            df_final.loc[:, campo] = df_final[campo].str.strip()\n",
    "            df_final.loc[:, campo] = df_final[campo].str.replace('\"','')\n",
    "            df_final.loc[:, campo] = df_final[campo].str.replace('\\t',' ')\n",
    "            df_final.loc[:, campo] = df_final[campo].str.replace('|','¬')\n",
    "        except:\n",
    "            num_error_tipo = num_error_tipo + 1\n",
    "            print('\\t', 'error tipo de dato (string) - no se puede convertir campo:', campo, sys.exc_info()[1])\n",
    "        \n",
    "    if num_error_tipo > 0:\n",
    "        print('\\t', 'validación tipo de datos (archivo original vs meta): False', '\\n')\n",
    "    else:\n",
    "        print('\\t', 'validación tipo de datos (archivo original vs meta): True', '\\n')\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### validacion_archivo_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion_archivo_final(df_final, df_original, meta, validacion_cols):\n",
    "    dict_val_arch = {}\n",
    "    list_cols_meta = validacion_cols['list_cols_dict']\n",
    "\n",
    "#================================================================================================\n",
    "#******              metricas calidad             ******\n",
    "    # name\n",
    "#    print('\\nValidación Archivo Final......')\n",
    "    name = {}\n",
    "    name['pre'] = nombre_archivo\n",
    "    name['post'] = '{}_{}_{}_{}{}'.format(meta['dominio'], meta['subdominio'],meta['fuente'], periodo, \n",
    "                                                       meta['extension'])\n",
    "    # num_rows \n",
    "    num_rows = {}\n",
    "    num_rows['pre'] = df_original.shape[0] + 1 # mas 1 por el encabezado\n",
    "    num_rows['post'] = df_final.shape[0] + 1 # mas 1 por el encabezado\n",
    "    num_rows['check'] = num_rows['pre'] == num_rows['post']\n",
    "\n",
    "    # num_cols\n",
    "    num_cols = {}\n",
    "    num_cols['pre'] = df_original.shape[1] \n",
    "    num_cols['post'] = df_final.shape[1] \n",
    "    num_cols['num_cols_faltantes_check'] = validacion_cols['list_cols_faltantes_check']\n",
    "    num_cols['list_cols_faltantes'] = validacion_cols['list_cols_faltantes']\n",
    "    num_cols['num_cols_adicionales_check'] = num_cols['pre'] == num_cols['post']\n",
    "    num_cols['list_cols_adicionales'] = validacion_cols['list_cols_adicionales'] \n",
    "    \n",
    "    # check\n",
    "    check = {}\n",
    "    check['num_rows_check'] = num_rows['pre'] == num_rows['post']\n",
    "    check['num_cols_faltantes_check'] = validacion_cols['list_cols_faltantes_check']\n",
    "\n",
    "#================================================================================================\n",
    "#******       metricas calidad por columna  (conteos, nulos, tipos)        ******\n",
    "    cols = {}\n",
    "    cols_false = 0\n",
    "\n",
    "    for col in list_cols_meta:\n",
    "        try:    \n",
    "            if df_final[col].dtype == 'object': # df_final[col].dtype\n",
    "                tipo = 'string'\n",
    "            elif df_final[col].dtype == 'Int64': #WARNING\n",
    "                tipo = 'integer'\n",
    "            elif df_final[col].dtype == 'float64':\n",
    "                tipo = 'float'\n",
    "            else: \n",
    "                tipo = 'date'\n",
    "\n",
    "            list_col_val = {}\n",
    "\n",
    "            list_col_val['nulls'] = {}\n",
    "            list_col_val['nulls']['pre'] =  df_original[col].isnull().sum() \n",
    "            list_col_val['nulls']['post'] =  df_final[col].isnull().sum()\n",
    "            nulls_check = list_col_val['nulls']['pre'] == list_col_val['nulls']['post'] \n",
    "\n",
    "            list_col_val['count'] = {}\n",
    "            list_col_val['count']['pre'] =  df_original[col].count() \n",
    "            list_col_val['count']['post'] =  df_final[col].count()\n",
    "            count_check = list_col_val['count']['pre'] == list_col_val['count']['post']\n",
    "\n",
    "            list_col_val['types'] = {}\n",
    "            list_col_val['types']['pre'] = meta['cols'][col]['type']\n",
    "            list_col_val['types']['post'] = tipo     \n",
    "            types_check = df_final[col].dtype == meta['cols'][col]['type_np'] \n",
    "\n",
    "            if nulls_check == True and count_check == True and types_check:\n",
    "                list_col_val['check'] = True\n",
    "            else:\n",
    "                list_col_val['check'] = False\n",
    "\n",
    "            cols[col] = list_col_val\n",
    "\n",
    "    #******       validación columnas         ******    \n",
    "            if cols[col]['check'] == False:\n",
    "                cols_false = cols_false + 1\n",
    "                #se puede crear una lista y agregar las columnas False\n",
    "        except:\n",
    "            list_col_val['check'] = False\n",
    "            cols_false = cols_false + 1\n",
    "            #se puede crear una lista y agregar las columnas False\n",
    "\n",
    "    if cols_false > 0:\n",
    "        cols_check = False\n",
    "    else:\n",
    "        cols_check = True\n",
    "\n",
    "    cols['cols_check'] = cols_check\n",
    "    print('\\t','errores encontrados:', cols_false, '\\n')\n",
    "    check['cols_check'] = cols_check\n",
    "    #================================================================================================\n",
    "    #******              log final             ******\n",
    "    # log final\n",
    "    dict_val_arch['name'] = name\n",
    "    dict_val_arch['check'] = check\n",
    "    dict_val_arch['num_rows'] = num_rows\n",
    "    dict_val_arch['num_cols'] = num_cols\n",
    "    dict_val_arch['cols'] = cols\n",
    "    dict_val_arch\n",
    "    #================================================================================================\n",
    "    return dict_val_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### write_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(meta, dict_val_arch, periodo):\n",
    "    json_path = path + meta['dominio'] + '_' + meta['subdominio'] + '_' + meta['fuente'] + '_' + periodo + '.json' \n",
    "    json_path_2 = 'D:/Fuentes/Planos/Logs/' + meta['dominio'] + '_' + meta['subdominio'] + '_' + meta['fuente'] + '_' + periodo + '.json' \n",
    "    archivo = dict_val_arch\n",
    "\n",
    "    class NpEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, np.bool_):             \n",
    "                return bool(obj)\n",
    "            else:\n",
    "                return super(NpEncoder, self).default(obj)\n",
    "# Your codes .... \n",
    "    x = json.dumps(archivo, cls=NpEncoder)\n",
    "\n",
    "    with open(json_path, 'w') as fp:\n",
    "        json.dump(archivo, fp, cls=NpEncoder)\n",
    "    print('\\t', json_path, '\\n')\n",
    "    with open(json_path_2, 'w') as fp:\n",
    "        json.dump(archivo, fp, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso Final\n",
    "\n",
    "Objetos de entrada:\n",
    "- validacion_cols (diccionario que devuelve la función 'validacion_columnas')\n",
    "- meta (diccionario que devuelve la función 'read_meta')\n",
    "- df_original (dataframe que devuelve la función 'read_file')\n",
    "- df_final (dataframe que devuelve la función 'transformacion_datos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio Ejecución......\n",
      "\t 2024-12-12 07:41:37.906195 \n",
      "\n",
      "Cargue Archivos......\n",
      "\t meta (diccionario): True\n",
      "\t archivo original: True \n",
      "\n",
      "Validación Columnas......\n",
      "\t validación columnas faltantes:  True\n",
      "\t lista columnas faltantes:  []\n",
      "\t lista columnas adicionales:  [] \n",
      "\n",
      "Validación Datos......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f5131ab5f393>:47: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df_final.loc[:, campo] = df_final[campo].str.replace('|','¬')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t validación tipo de datos (archivo original vs meta): True \n",
      "\n",
      "Validación Final......\n",
      "\t errores encontrados: 0 \n",
      "\n",
      "Generación Archivo Final......\n",
      "\t D:/Fuentes/Planos/Diccionario-TRX/transacciones_diccionario-trx-emp_ods_H201910-202411.csv \n",
      "\n",
      "Generación Log......\n",
      "\t D:/Fuentes/Planos/Diccionario-TRX/transacciones_diccionario-trx-emp_ods_H201910-202411.json \n",
      "\n",
      "Generación Load......\n",
      "\t D:/Fuentes/Planos/Diccionario-TRX/transacciones_diccionario-trx-emp_ods_H201910-202411.load\n",
      "{\n",
      "\t\"entity\": \"BOCC\",\n",
      "\t\"filename\": \"transacciones_diccionario-trx-emp_ods_H201910-202411.csv\",\n",
      "\t\"domain\": \"transacciones\",\n",
      "\t\"subdomain\": \"diccionario-trx-emp\",\n",
      "\t\"source\": \"ods\",\n",
      "\t\"bucket_name_dl\": \"data-bocc-pro-landing\",\n",
      "\t\"generation_date\": \"2024-12-12\",\n",
      "\t\"start_period\": \"2019-10\",\n",
      "\t\"end_period\": \"2024-11\",\n",
      "\t\"frequency\": \"HISTORICAL\",\n",
      "\t\"num_rows\": 223433,\n",
      "\t\"size\": \"12761438\",\n",
      "\t\"meta_file\": \"transacciones_diccionario-trx-emp_ods_v0-0.meta\",\n",
      "\t\"version\": \"0-0\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>str_nombre_transaccion</th>\n",
       "      <th>str_naturaleza_transaccion</th>\n",
       "      <th>tipo_transaccion</th>\n",
       "      <th>tipo_movimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UAE COUNCILOR IN COLO</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE HOME DEPOT #6586  ESS</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TASHAS CANAL WALK     URY CITY</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TARGET T-2848         I</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TARGET T-2171         NTA</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223427</th>\n",
       "      <td>SQ *CARACAS BAKERY BI i</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223428</th>\n",
       "      <td>MIS PARRILLA EL ENSUE A</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223429</th>\n",
       "      <td>ALISTCAR              A</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223430</th>\n",
       "      <td>BOLD*Bluetech think</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223431</th>\n",
       "      <td>ARQUIVIDRIOS Y ALUMIN AGENA</td>\n",
       "      <td>D</td>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>OPERACIONAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223432 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                str_nombre_transaccion str_naturaleza_transaccion  \\\n",
       "0                UAE COUNCILOR IN COLO                          D   \n",
       "1            THE HOME DEPOT #6586  ESS                          D   \n",
       "2       TASHAS CANAL WALK     URY CITY                          D   \n",
       "3              TARGET T-2848         I                          D   \n",
       "4            TARGET T-2171         NTA                          D   \n",
       "...                                ...                        ...   \n",
       "223427         SQ *CARACAS BAKERY BI i                          D   \n",
       "223428         MIS PARRILLA EL ENSUE A                          D   \n",
       "223429         ALISTCAR              A                          D   \n",
       "223430             BOLD*Bluetech think                          D   \n",
       "223431     ARQUIVIDRIOS Y ALUMIN AGENA                          D   \n",
       "\n",
       "       tipo_transaccion tipo_movimiento  \n",
       "0       ESTABLECIMIENTO     OPERACIONAL  \n",
       "1       ESTABLECIMIENTO     OPERACIONAL  \n",
       "2       ESTABLECIMIENTO     OPERACIONAL  \n",
       "3       ESTABLECIMIENTO     OPERACIONAL  \n",
       "4       ESTABLECIMIENTO     OPERACIONAL  \n",
       "...                 ...             ...  \n",
       "223427  ESTABLECIMIENTO     OPERACIONAL  \n",
       "223428  ESTABLECIMIENTO     OPERACIONAL  \n",
       "223429  ESTABLECIMIENTO     OPERACIONAL  \n",
       "223430  ESTABLECIMIENTO     OPERACIONAL  \n",
       "223431  ESTABLECIMIENTO     OPERACIONAL  \n",
       "\n",
       "[223432 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Inicio Ejecución......') \n",
    "print('\\t', datetime.now(), '\\n')\n",
    "\n",
    "#******  cargue de meadata necesaria (dict_meta)  ******\n",
    "print('Cargue Archivos......')\n",
    "meta = read_meta(file_path_dict)\n",
    "print('\\t', 'meta (diccionario): True')\n",
    "\n",
    "#******  cargue archivo inicial/original  ******\n",
    "df_original = read_file(file_path)\n",
    "print('\\t', 'archivo original: True', '\\n')\n",
    "\n",
    "#******  validación columnas ******\n",
    "print('Validación Columnas......')\n",
    "validacion_cols = validacion_columnas(df_original)\n",
    "print('\\t', 'validación columnas faltantes: ',validacion_cols['list_cols_faltantes_check'] )\n",
    "print('\\t', 'lista columnas faltantes: ',validacion_cols['list_cols_faltantes'] )\n",
    "print('\\t', 'lista columnas adicionales: ',validacion_cols['list_cols_adicionales'], '\\n')\n",
    "\n",
    "#******  transformación datos  ****** !!!!!REVISAR WARNINGS!!!!!\n",
    "print('Validación Datos......')\n",
    "pd.options.mode.chained_assignment = None  # default='warn' - desactiva warning\n",
    "df_final = transformacion_datos(df_original, meta)\n",
    "pd.options.mode.chained_assignment = 'warn'  # default='warn' - activa warning\n",
    "\n",
    "#******  validación datos  ****** \n",
    "print('Validación Final......')\n",
    "dict_val_arch = validacion_archivo_final(df_final, df_original, meta, validacion_cols)\n",
    "\n",
    "#******  validación archivo final ****** \n",
    "check_false = 0\n",
    "for x in dict_val_arch['check']:\n",
    "    if dict_val_arch['check'][x] == False:\n",
    "        check_false = check_false + 1\n",
    "        #print('\\t', 'error en:', x)\n",
    "\n",
    "#******  export archivo final, log y load ****** \n",
    "if check_false == 0:\n",
    "    print('Generación Archivo Final......')\n",
    "    file_export = path + dict_val_arch['name']['post']\n",
    "    print('\\t', file_export, '\\n')\n",
    "    df_final.to_csv(file_export, sep = '|', encoding = 'utf-8', \n",
    "                decimal = ',', doublequote = False, index = False, date_format='%d/%m/%Y')\n",
    "    \n",
    "    print('Generación Log......')\n",
    "    write_json(meta, dict_val_arch, periodo)\n",
    "    print('Generación Load......')\n",
    "    generar_load(file_export)\n",
    "else:\n",
    "    print('Generación Log......')\n",
    "    write_json(meta, dict_val_arch, periodo)\n",
    "    print('============= No se genera archivo =============')\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
